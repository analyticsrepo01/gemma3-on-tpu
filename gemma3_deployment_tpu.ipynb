{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "SgQ6t5bqZVlH"
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99c1c3fc2ca5"
   },
   "source": [
    "# Vertex AI Model Garden - Gemma 3 Deployment on Trillium TPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "264c07757582"
   },
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27832,
     "status": "ok",
     "timestamp": 1755669467073,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "VrQvw5wN3gzl",
    "outputId": "50b0194d-1b84-42af-ef8c-38df4ffef2ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'vertex-ai-samples' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-07 09:48:19.923831: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757238499.950123   12050 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757238499.957738   12050 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1757238499.977676   12050 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757238499.977711   12050 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757238499.977715   12050 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757238499.977717   12050 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-09-07 09:48:19.984859: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling Vertex AI API and Compute Engine API.\n",
      "Operation \"operations/acat.p2-87995179092-dd49b808-e449-41b7-97e6-2c9bb1a0e83c\" finished successfully.\n",
      "Using this GCS Bucket: gs://llama31_training-europe\n",
      "Initializing Vertex AI API.\n",
      "Using this default Service Account: 87995179092-compute@developer.gserviceaccount.com\n",
      "No changes made to gs://llama31_training-europe/\n",
      "Updated property [core/project].\n",
      " [1] EXPRESSION=request.time < timestamp(\"2025-05-26T15:00:14.127Z\"), TITLE=cloudbuild-connection-setup\n",
      " [2] None\n",
      " [3] Specify a new condition\n",
      "The policy contains bindings with conditions, so specifying a condition is \n",
      "required when adding a binding. Please specify a condition.:  ^C\n",
      " [1] EXPRESSION=request.time < timestamp(\"2025-05-26T15:00:14.127Z\"), TITLE=cloudbuild-connection-setup\n",
      " [2] None\n",
      " [3] Specify a new condition\n",
      "The policy contains bindings with conditions, so specifying a condition is \n",
      "required when adding a binding. Please specify a condition.:  ^C\n"
     ]
    }
   ],
   "source": [
    "# @title Setup Google Cloud project\n",
    "\n",
    "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "# @markdown 2. **[Optional]** [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs. Set the BUCKET_URI for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the same region as where the notebook was launched. Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\"). If not set, a unique GCS bucket will be created instead.\n",
    "\n",
    "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = \"gs://llama31_training-europe\"  # @param {type:\"string\"}\n",
    "\n",
    "# @markdown 3. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
    "\n",
    "REGION = TPU_DEPLOYMENT_REGION = \"europe-west4\"  # @param {type:\"string\"}\n",
    "\n",
    "# Upgrade Vertex AI SDK.\n",
    "! pip3 install --upgrade --quiet 'google-cloud-aiplatform>=1.64.0'\n",
    "\n",
    "# Import the necessary packages\n",
    "import datetime\n",
    "import importlib\n",
    "import os\n",
    "import uuid\n",
    "from typing import Tuple\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
    "\n",
    "models, endpoints = {}, {}\n",
    "\n",
    "common_util = importlib.import_module(\n",
    "    \"vertex-ai-samples.notebooks.community.model_garden.docker_source_codes.notebook_util.common_util\"\n",
    ")\n",
    "\n",
    "# Get the default cloud project id.\n",
    "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
    "\n",
    "# Get the default region for launching jobs.\n",
    "if not REGION:\n",
    "    if not os.environ.get(\"GOOGLE_CLOUD_REGION\"):\n",
    "        raise ValueError(\n",
    "            \"REGION must be set. See\"\n",
    "            \" https://cloud.google.com/vertex-ai/docs/general/locations for\"\n",
    "            \" available cloud locations.\"\n",
    "        )\n",
    "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
    "\n",
    "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
    "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
    "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
    "\n",
    "# Cloud Storage bucket for storing the experiment artifacts.\n",
    "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
    "# prefer using your own GCS bucket, change the value yourself below.\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
    "\n",
    "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
    "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}-{str(uuid.uuid4())[:4]}\"\n",
    "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
    "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
    "else:\n",
    "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
    "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
    "    bucket_region = shell_output[0].strip().lower()\n",
    "    if bucket_region != REGION:\n",
    "        raise ValueError(\n",
    "            \"Bucket region %s is different from notebook region %s\"\n",
    "            % (bucket_region, REGION)\n",
    "        )\n",
    "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
    "\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "MODEL_BUCKET = os.path.join(BUCKET_URI, \"vllm_tpu\")\n",
    "\n",
    "\n",
    "# Initialize Vertex AI API.\n",
    "print(\"Initializing Vertex AI API.\")\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
    "\n",
    "# Gets the default SERVICE_ACCOUNT.\n",
    "shell_output = ! gcloud projects describe $PROJECT_ID\n",
    "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
    "\n",
    "\n",
    "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
    "\n",
    "! gcloud config set project $PROJECT_ID\n",
    "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/storage.admin\"\n",
    "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/aiplatform.user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YXFGIp1l-qtT"
   },
   "outputs": [],
   "source": [
    "# @title Access the models\n",
    "# @markdown ### Access gemma-3-27b-it models on Vertex AI for serving\n",
    "# @markdown The models from the Hugging Face can be used for serving in Vertex AI.\n",
    "# @markdown 1. Open the [gemma-3-27b-it](https://huggingface.co/google/gemma-3-27b-it).\n",
    "# @markdown 2. Review and accept the agreement.\n",
    "# @markdown 3. After accepting the agreement, models will be available for serving.\n",
    "# @markdown 4. You must provide a Hugging Face User Access Token (with read access) to access the Llama 3.1 model. You can follow the [Hugging Face documentation](https://huggingface.co/docs/hub/en/security-tokens) to create a **read** access token and put it in the `HF_TOKEN` field below.\n",
    "\n",
    "HF_TOKEN = \"hf_here\"  # @param {type:\"string\", isTemplate:true}\n",
    "if not HF_TOKEN:\n",
    "    print(\"Provide a read HF_TOKEN to Gemma 3 models from Hugging Face\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1755731418807,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "7MgDO17YG7nz",
    "outputId": "38e78a5b-d23a-42a5-c3d4-5e46d4f5c959"
   },
   "outputs": [],
   "source": [
    "# @title Prepare\n",
    "\n",
    "MODEL_ID = \"gemma-3-27b-it\"\n",
    "model_path_prefix = \"google/\"\n",
    "model_id = os.path.join(model_path_prefix, MODEL_ID)\n",
    "model_publisher = \"google\"\n",
    "model_publisher_id = \"gemma3\"\n",
    "machine_type = \"ct6e-standard-4t\"\n",
    "tpu_count = 4\n",
    "tpu_topo = \"2x2\"\n",
    "tpu_type = \"TPU_V6e\"\n",
    "\n",
    "vLLM_TPU_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20250819_0917_tpu_experimental_RC01\"\n",
    "# @markdown Set `use_dedicated_endpoint` to False if you don't want to use [dedicated endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment#create-dedicated-endpoint).\n",
    "use_dedicated_endpoint = True  # @param {type:\"boolean\"}\n",
    "\n",
    "# Server parameters.\n",
    "tensor_parallel_size = tpu_count\n",
    "\n",
    "# Maximum context length for a request.\n",
    "max_model_len = 8192  # @param\n",
    "\n",
    "# Endpoint configurations.\n",
    "min_replica_count = 1\n",
    "max_replica_count = 1\n",
    "\n",
    "run_name = \"test-deployment\"  # @param {type:\"string\"}\n",
    "\n",
    "# @markdown Note: The vLLM-TPU container used in this notebook is in experimental status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctdQJhrdaeeT"
   },
   "source": [
    "## Deploy prebuilt Llama 3.1 8B or Qwen3 32B models with vLLM on TPUs\n",
    "This section will download the prebuilt model chosen in the previous section and deploys it to a Vertex AI Endpoint. It takes 15 minutes to 1 hour to finish depending on the size of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Cwf_xWpEFkWL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/87995179092/locations/europe-west4/endpoints/8789946752208732160/operations/300970554821705728\n",
      "Endpoint created. Resource name: projects/87995179092/locations/europe-west4/endpoints/8789946752208732160\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/87995179092/locations/europe-west4/endpoints/8789946752208732160')\n",
      "Creating Model\n",
      "Create Model backing LRO: projects/87995179092/locations/europe-west4/models/5957068237383401472/operations/2471705575214284800\n",
      "Model created. Resource name: projects/87995179092/locations/europe-west4/models/5957068237383401472@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/87995179092/locations/europe-west4/models/5957068237383401472@1')\n",
      "Deploying model to Endpoint : projects/87995179092/locations/europe-west4/endpoints/8789946752208732160\n",
      "Deploy Endpoint model backing LRO: projects/87995179092/locations/europe-west4/endpoints/8789946752208732160/operations/3120223921555636224\n",
      "Endpoint model deployed. Resource name: projects/87995179092/locations/europe-west4/endpoints/8789946752208732160\n"
     ]
    }
   ],
   "source": [
    "# @title Deploy\n",
    "def deploy_model_vllm_tpu(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    publisher: str,\n",
    "    publisher_model_id: str,\n",
    "    service_account: str,\n",
    "    base_model_id: str = None,\n",
    "    tensor_parallel_size: int = 1,\n",
    "    machine_type: str = \"ct6e-standard-1t\",\n",
    "    tpu_topology: str = \"1x1\",\n",
    "    max_model_len: int = 4096,\n",
    "    enable_chunked_prefill: bool = False,\n",
    "    enable_prefix_cache: bool = False,\n",
    "    endpoint_id: str = \"\",\n",
    "    min_replica_count: int = 1,\n",
    "    max_replica_count: int = 1,\n",
    "    use_dedicated_endpoint: bool = False,\n",
    "    model_type: str = None,\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys models with vLLM on TPU in Vertex AI.\"\"\"\n",
    "    if endpoint_id:\n",
    "        aip_endpoint_name = (\n",
    "            f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_id}\"\n",
    "        )\n",
    "        endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
    "    else:\n",
    "        endpoint = aiplatform.Endpoint.create(\n",
    "            display_name=f\"{model_name}-endpoint\",\n",
    "            location=TPU_DEPLOYMENT_REGION,\n",
    "            dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
    "        )\n",
    "\n",
    "    if not base_model_id:\n",
    "        base_model_id = model_id\n",
    "\n",
    "    if not tensor_parallel_size:\n",
    "        tensor_parallel_size = int(machine_type[-2])\n",
    "\n",
    "    num_hosts = int(tpu_topology.split(\"x\")[0])\n",
    "\n",
    "    vllmtpu_args = [\n",
    "        \"python\",\n",
    "        \"-m\",\n",
    "        \"vllm.entrypoints.api_server\",\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--model={model_id}\",\n",
    "        f\"--tensor_parallel_size={tensor_parallel_size}\",\n",
    "        f\"--max_model_len={max_model_len}\",\n",
    "        \"--limit_mm_per_prompt.image=0\",\n",
    "    ]\n",
    "\n",
    "    if enable_chunked_prefill:\n",
    "        vllmtpu_args.append(\"--enable-chunked-prefill\")\n",
    "\n",
    "    if enable_prefix_cache:\n",
    "        vllmtpu_args.append(\"--enable-prefix-caching\")\n",
    "\n",
    "    env_vars = {\n",
    "        \"MODEL_ID\": base_model_id,\n",
    "        \"DEPLOY_SOURCE\": \"notebook\",\n",
    "        \"VLLM_USE_V1\": \"1\",\n",
    "    }\n",
    "\n",
    "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
    "    try:\n",
    "        if HF_TOKEN:\n",
    "            env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=vLLM_TPU_DOCKER_URI,\n",
    "        serving_container_args=vllmtpu_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_environment_variables=env_vars,\n",
    "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
    "        serving_container_deployment_timeout=4500,\n",
    "        model_garden_source_model_name=(\n",
    "            f\"publishers/{publisher}/models/{publisher_model_id}\"\n",
    "        ),\n",
    "        location=TPU_DEPLOYMENT_REGION,\n",
    "    )\n",
    "\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        tpu_topology=tpu_topology if num_hosts > 1 else None,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "        min_replica_count=min_replica_count,\n",
    "        max_replica_count=max_replica_count,\n",
    "        system_labels={\n",
    "            \"NOTEBOOK_NAME\": \"model_garden_pytorch_llama3_1_qwen3_deployment_tpu.ipynb\",\n",
    "        },\n",
    "    )\n",
    "    return model, endpoint\n",
    "\n",
    "\n",
    "models[\"vllmtpu\"], endpoints[\"vllmtpu\"] = deploy_model_vllm_tpu(\n",
    "    model_name=common_util.get_job_name_with_datetime(prefix=run_name),\n",
    "    model_id=model_id,\n",
    "    publisher=model_publisher,\n",
    "    publisher_model_id=model_publisher_id,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    tensor_parallel_size=tensor_parallel_size,\n",
    "    machine_type=machine_type,\n",
    "    tpu_topology=tpu_topo,\n",
    "    max_model_len=max_model_len,\n",
    "    enable_chunked_prefill=False,\n",
    "    enable_prefix_cache=False,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1471,
     "status": "ok",
     "timestamp": 1755733848575,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "78nk6Cl2pqer",
    "outputId": "68dc9345-e677-4a5a-b5d3-60b6eaef459d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "What is a car?\n",
      "Output:\n",
      " A car is a vehicle that has the ability to transport people or goods from one place to another. They typically have four wheels and operate on roads.\n",
      "\n",
      "But the concept of a car has evolved over time.\n",
      "\n",
      "Here's a more comprehensive look\n"
     ]
    }
   ],
   "source": [
    "# @title Raw predict\n",
    "\n",
    "# @markdown Once deployment succeeds, you can send requests to the endpoint with text prompts. Sampling parameters supported by vLLM can be found [here](https://docs.vllm.ai/en/latest/dev/sampling_params.html).\n",
    "\n",
    "# @markdown Example:\n",
    "\n",
    "# @markdown ```\n",
    "# @markdown Human: What is a car?\n",
    "# @markdown Assistant:  A car, or a motor car, is a road-connected human-transportation system used to move people or goods from one place to another. The term also encompasses a wide range of vehicles, including motorboats, trains, and aircrafts. Cars typically have four wheels, a cabin for passengers, and an engine or motor. They have been around since the early 19th century and are now one of the most popular forms of transportation, used for daily commuting, shopping, and other purposes.\n",
    "# @markdown ```\n",
    "# @markdown Additionally, you can moderate the generated text with Vertex AI. See [Moderate text documentation](https://cloud.google.com/natural-language/docs/moderating-text) for more details.\n",
    "\n",
    "# Loads an existing endpoint instance using the endpoint name:\n",
    "# - Using `endpoint_name = endpoint.name` allows us to get the\n",
    "#   endpoint name of the endpoint `endpoint` created in the cell\n",
    "#   above.\n",
    "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
    "#   an existing endpoint with the ID 1234567890123456789.\n",
    "# You may uncomment the code below to load an existing endpoint.\n",
    "\n",
    "# endpoint_name = \"\"  # @param {type:\"string\"}\n",
    "# aip_endpoint_name = (\n",
    "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "# )\n",
    "# endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
    "\n",
    "prompt = \"What is a car?\"  # @param {type: \"string\"}\n",
    "# @markdown If you encounter an issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, by lowering `max_tokens`.\n",
    "max_tokens = 50  # @param {type:\"integer\"}\n",
    "temperature = 1.0  # @param {type:\"number\"}\n",
    "\n",
    "# @markdown Set `raw_response` to `True` to obtain the raw model output. Set `raw_response` to `False` to apply additional formatting in the structure of `\"Prompt:\\n{prompt.strip()}\\nOutput:\\n{output}\"`.\n",
    "raw_response = False  # @param {type:\"boolean\"}\n",
    "\n",
    "# Overrides parameters for inferences.\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"raw_response\": raw_response,\n",
    "    },\n",
    "]\n",
    "response = endpoints[\"vllmtpu\"].predict(\n",
    "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
    ")\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)\n",
    "# @markdown Note Top-k sampling is not currently enabled for vLLM on TPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxaKC69ypQds"
   },
   "source": [
    "## Clean up resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "id": "HYNmBz8PdcvJ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undeploying Endpoint model: projects/87995179092/locations/europe-west4/endpoints/8789946752208732160\n",
      "Undeploy Endpoint model backing LRO: projects/87995179092/locations/europe-west4/endpoints/8789946752208732160/operations/2723907154347032576\n",
      "Endpoint model undeployed. Resource name: projects/87995179092/locations/europe-west4/endpoints/8789946752208732160\n",
      "Deleting Endpoint : projects/87995179092/locations/europe-west4/endpoints/8789946752208732160\n",
      "Endpoint deleted. . Resource name: projects/87995179092/locations/europe-west4/endpoints/8789946752208732160\n",
      "Deleting Endpoint resource: projects/87995179092/locations/europe-west4/endpoints/8789946752208732160\n",
      "Delete Endpoint backing LRO: projects/87995179092/locations/europe-west4/operations/1451077309661446144\n",
      "Endpoint resource projects/87995179092/locations/europe-west4/endpoints/8789946752208732160 deleted.\n",
      "Deleting Model : projects/87995179092/locations/europe-west4/models/5957068237383401472\n",
      "Model deleted. . Resource name: projects/87995179092/locations/europe-west4/models/5957068237383401472\n",
      "Deleting Model resource: projects/87995179092/locations/europe-west4/models/5957068237383401472\n",
      "Delete Model backing LRO: projects/87995179092/locations/europe-west4/models/5957068237383401472/operations/7794960334766211072\n",
      "Model resource projects/87995179092/locations/europe-west4/models/5957068237383401472 deleted.\n"
     ]
    }
   ],
   "source": [
    "# @title Delete the models and endpoints\n",
    "# @markdown  Delete the experiment models and endpoints to recycle the resources\n",
    "# @markdown  and avoid unnecessary continuous charges that may incur.\n",
    "\n",
    "# Undeploy model and delete endpoint.\n",
    "for endpoint in endpoints.values():\n",
    "    endpoint.delete(force=True)\n",
    "\n",
    "# Delete models.\n",
    "for model in models.values():\n",
    "    model.delete()\n",
    "\n",
    "delete_bucket = False  # @param {type:\"boolean\"}\n",
    "if delete_bucket:\n",
    "    ! gsutil -m rm -r $BUCKET_NAME"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model_garden_pytorch_llama3_1_qwen3_deployment_tpu.ipynb",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
